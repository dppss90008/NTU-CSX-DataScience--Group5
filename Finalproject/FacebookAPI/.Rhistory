doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
ssion_redirected = rvest::submit_form(session = session, form = form )
time <- session_redirected %>%
html_nodes(".article-metaline+ .article-metaline .article-meta-value") %>% html_text
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
print(hour)
}
doc
sapply(url, getdoc)
# result <- FindURL("https://www.ptt.cc/bbs/Gossiping/search?page=1&q=%E6%9F%AF%E6%96%87%E5%93%B2")
# View(result)
getdoc <- function(x)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = x)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
ssion_redirected = rvest::submit_form(session = session, form = form )
time <- session_redirected %>%
html_nodes(".article-metaline+ .article-metaline .article-meta-value") %>% html_text
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
print(hour)
}
sapply(url, getdoc)
# result <- FindURL("https://www.ptt.cc/bbs/Gossiping/search?page=1&q=%E6%9F%AF%E6%96%87%E5%93%B2")
# View(result)
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
ssion_redirected = rvest::submit_form(session = session, form = form )
time <- session_redirected %>%
html_nodes(".article-metaline+ .article-metaline .article-meta-value") %>% html_text
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
print(hour)
}
sapply(x, getdoc)
library(rvest)
library(magrittr)
library(httr)
library(dplyr)
data =read_html("https://www.ptt.cc/bbs/Gossiping/search?page=1&q=%E6%9F%AF%E6%96%87%E5%93%B2") %>% html_text(trim = T)
prefix <- "https://www.ptt.cc/bbs/Gossiping/search?page="
FindURL <- function(URL){
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
URLlink <- session_redirected %>%
html_nodes(".title a") %>% html_attr(.,"href")
title <- session_redirected %>%
html_nodes(".title a") %>% html_text
Date <- session_redirected %>%
html_nodes(".date") %>% html_text
output <- cbind(title,URLlink,Date)
return(output)
}
drko <- data.frame()
for(i in c(1:10))
{
x <- paste0(prefix, i , "&q=%E6%9F%AF%E6%96%87%E5%93%B2")
drko2 <- rbind(drko, FindURL(x))
}
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
ssion_redirected = rvest::submit_form(session = session, form = form )
time <- session_redirected %>%
html_nodes(".article-metaline+ .article-metaline .article-meta-value") %>% html_text
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
print(hour)
}
sapply(x, getdoc)
# result <- FindURL("https://www.ptt.cc/bbs/Gossiping/search?page=1&q=%E6%9F%AF%E6%96%87%E5%93%B2")
# View(result)
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
}
sapply(x, getdoc)
library(rvest)
library(magrittr)
library(httr)
library(dplyr)
data =read_html("https://www.ptt.cc/bbs/Gossiping/search?page=1&q=%E6%9F%AF%E6%96%87%E5%93%B2") %>% html_text(trim = T)
prefix <- "https://www.ptt.cc/bbs/Gossiping/search?page="
FindURL <- function(URL){
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
URLlink <- session_redirected %>%
html_nodes(".title a") %>% html_attr(.,"href")
title <- session_redirected %>%
html_nodes(".title a") %>% html_text
Date <- session_redirected %>%
html_nodes(".date") %>% html_text
output <- cbind(title,URLlink,Date)
return(output)
}
drko <- data.frame()
for(i in c(1:10))
{
x <- paste0(prefix, i , "&q=%E6%9F%AF%E6%96%87%E5%93%B2")
drko2 <- rbind(drko, FindURL(x))
}
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
time <- session_redirected %>%
html_nodes(".article-metaline+ .article-metaline .article-meta-value") %>% html_text
}
sapply(x, getdoc)
sapply(x, getdoc)
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
}
sapply(x, getdoc)
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
}
sapply(x, getdoc)
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
}
sapply(x, getdoc)
sapply(x, getdoc)
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
}
sapply(x, getdoc)
# ===== 文字雲程式 : 臉書文字雲
# ===== 處理姚文智1月至5月的文章
# ------------------------------------------
#
# 匯入套件
library(tibble)
library(dplyr)
library(tidyr)
library(data.table)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(magrittr)
library(RColorBrewer)
library(wordcloud)
# 匯入資料組
setwd("/Users/Weber/Documents/GitHub/NTU-CSX-DataScience--Group5/Finalproject/FacebookAPI")
data <- read.csv("Yao_report.csv", encoding = "UTF-8")
# 清除NA
data <- data %>% na.omit()
# 切開時間
data <- data %>% separate(time, c("year","month","day"),"-")
data <- data %>% separate(day, c("date","time"), "T")
# 排序
data <- data[with(data, order(year ,month, date)), ]
# 清理重複資料
data <- data[!duplicated(data$post), ]
# 移除2017資料
data <- data[data$year == "2018",]
row.names(data) = c(1:235) # 由資料數重新編排號碼
# 依月份建立子資料組
Y1<- subset(data , data$month == "01", select = post)
Y2<- subset(data , data$month == "02", select = post)
Y3<- subset(data , data$month == "03", select = post)
Y4<- subset(data , data$month == "04", select = post)
Y5<- subset(data , data$month == "05", select = post)
Y6<- subset(data , data$month == "06", select = post)
# ---- 2018
docs <- Corpus(VectorSource(data$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("分享","記者","攝影","提及","表示","報導","我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 清除單字
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) == 1){
freqFrame[i,] <- NA
}
}
freqFrame <- na.omit(freqFrame)
# ---- Jan
docs <- Corpus(VectorSource(Y1$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","》","♀","♂","『","』","【","】","／","＼","：","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
# Error in file.exists(code[1]) : file name conversion problem -- name too long?
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 清除單字
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) == 1){
freqFrame[i,] <- NA
}
}
freqFrame <- na.omit(freqFrame)
# 畫出文字雲
# 儲存文字雲圖片
png("Yao_Jan.png", width = 300, height = 300 )
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=5,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
# Error in strwidth(words[i], cex = size[i], ...) : 'cex'
dev.off()
# ---- Feb
docs <- Corpus(VectorSource(Y2$post))
View(freqFrame)
View(seg)
View(data)
data <- read.csv("Yao_report.csv", encoding = "big5")
View(data)
# 清除NA
data <- data %>% na.omit()
# 切開時間
data <- data %>% separate(time, c("year","month","day"),"-")
data <- data %>% separate(day, c("date","time"), "T")
# 排序
data <- data[with(data, order(year ,month, date)), ]
# 清理重複資料
data <- data[!duplicated(data$post), ]
# 移除2017資料
data <- data[data$year == "2018",]
row.names(data) = c(1:235) # 由資料數重新編排號碼
# 依月份建立子資料組
Y1<- subset(data , data$month == "01", select = post)
Y2<- subset(data , data$month == "02", select = post)
Y3<- subset(data , data$month == "03", select = post)
Y4<- subset(data , data$month == "04", select = post)
Y5<- subset(data , data$month == "05", select = post)
Y6<- subset(data , data$month == "06", select = post)
docs <- Corpus(VectorSource(Y1$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","》","♀","♂","『","』","【","】","／","＼","：","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
# Error in file.exists(code[1]) : file name conversion problem -- name too long?
seg = lapply(docs, jieba_tokenizer)
seg
# ===== 文字雲程式 : 臉書文字雲
# ===== 處理姚文智1月至5月的文章
# ------------------------------------------
#
# 匯入套件
library(tibble)
library(dplyr)
library(tidyr)
library(data.table)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(magrittr)
library(RColorBrewer)
library(wordcloud)
# 匯入資料組
setwd("/Users/Weber/Documents/GitHub/NTU-CSX-DataScience--Group5/Finalproject/FacebookAPI")
data <- read.csv("Yao_report.csv", encoding = "big5")
# 清除NA
data <- data %>% na.omit()
# 切開時間
data <- data %>% separate(time, c("year","month","day"),"-")
data <- data %>% separate(day, c("date","time"), "T")
# 排序
data <- data[with(data, order(year ,month, date)), ]
# 清理重複資料
data <- data[!duplicated(data$post), ]
# 移除2017資料
data <- data[data$year == "2018",]
row.names(data) = c(1:235) # 由資料數重新編排號碼
# 依月份建立子資料組
Y1<- subset(data , data$month == "01", select = post)
Y2<- subset(data , data$month == "02", select = post)
Y3<- subset(data , data$month == "03", select = post)
Y4<- subset(data , data$month == "04", select = post)
Y5<- subset(data , data$month == "05", select = post)
Y6<- subset(data , data$month == "06", select = post)
# debug
# Y1 <- Y1[-23,]
# Di_text <- matrix(data = NA, nrow = 67,ncol = 5 )
# Di_text <- cbind(data_1$bindtext, data_2$bindtext, data_3$bindtext, data_4$bindtext, data_5$bindtext)
# ----------------------------------------------------------------------------------------------------------
# 清理文本內容，刪除文本中不需要的雜訊
# data_i$bindtext <- substr(data$bindtext, 69, (nchar(as.vector(data$bindtext))-183))
# data$bindtext <- substr(data$bindtext, 69, (nchar(as.vector(data$bindtext))-183))
# 切詞
# data_1$bindtext[1:3] <- substr(data_1$bindtext[1:3], 69, (nchar(as.vector(data_1$bindtext[1:3]))-176))
# ----------------------------------------------------------------------------------------------------------
# 以上內容有待開發
# ==== 姚文智
# ---- Jan
docs <- Corpus(VectorSource(Y1$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","》","♀","♂","『","』","【","】","／","＼","：","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
# Error in file.exists(code[1]) : file name conversion problem -- name too long?
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 清除單字
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) == 1){
freqFrame[i,] <- NA
}
}
freqFrame <- na.omit(freqFrame)
View(docs)
View(Y1)
View(docs)
