# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 清除單字
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) == 1){
freqFrame[i,] <- NA
}
}
freqFrame <- na.omit(freqFrame)
View(freqFrame)
png("Yao_May.png", width = 300, height = 300 )
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=12,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
dev.off()
# ---- Jun
docs <- Corpus(VectorSource(Y6$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("分享","記者","攝影","提及","表示","報導","我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 清除單字
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) == 1){
freqFrame[i,] <- NA
}
}
freqFrame <- na.omit(freqFrame)
View(freqFrame)
png("Di_Jun.png", width = 300, height = 300 )
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=5,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
dev.off()
png("Yao_Jun.png", width = 300, height = 300 )
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=5,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
dev.off()
# ---- 2018
docs <- Corpus(VectorSource(data$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("分享","記者","攝影","提及","表示","報導","我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 清除單字
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) == 1){
freqFrame[i,] <- NA
}
}
freqFrame <- na.omit(freqFrame)
# 畫出文字雲
Yao_textNum <- rbind(nrow(Y1),nrow(Y2),nrow(Y3),nrow(Y4),nrow(Y5),nrow(Y6) )
Yao_textNum <- Yao_textNum %>% as.data.frame()
colnames(Yao_textNum) <- "姚文智報導量"
rownames(Yao_textNum) <- c("Jan","Feb","Mar","Apr","May","Jun")
Yao_textNum
# 輸出
write.table(Yao_textNum, file = "Yao_textNum.CSV", sep = ",")
View(Y1)
View(Y2)
View(Y3)
View(Y4)
View(Y5)
docs <- Corpus(VectorSource(Yao1$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","＼","：","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
docs <- Corpus(VectorSource(Y1$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","＼","：","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
# 有詞頻之後就可以去畫文字雲
x <- 0
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
x <- x + 1
print(x)
}
# Error in file.exists(code[1]) : file name conversion problem -- name too long?
seg = lapply(docs, jieba_tokenizer)
# debug
Y1 <- Y1[-23]
View(Y1)
# debug
Y1 <- Y1[-23,]
# 依月份建立子資料組
Y1<- subset(data , data$month == "01", select = post)
View(Y1)
# debug
Y1 <- Y1[-23,]
View(Y1)
length(Y1)
docs <- Corpus(VectorSource(Y1$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","＼","：","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
# 有詞頻之後就可以去畫文字雲
# x <- 0
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
# x <- x + 1
# print(x)
}
# Error in file.exists(code[1]) : file name conversion problem -- name too long?
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 清除單字
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) == 1){
freqFrame[i,] <- NA
}
}
freqFrame <- na.omit(freqFrame)
# 有詞頻之後就可以去畫文字雲
x <- 0
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
x <- x + 1
print(x)
}
# Error in file.exists(code[1]) : file name conversion problem -- name too long?
seg = lapply(docs, jieba_tokenizer)
View(Y1)
# 依月份建立子資料組
Y1<- subset(data , data$month == "01", select = post)
View(Y1)
docs <- Corpus(VectorSource(Y1$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","＼","：","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
for (i in 1:24) {
print(i)
}
}
# Error in file.exists(code[1]) : file name conversion problem -- name too long?
seg = lapply(docs, jieba_tokenizer)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
x <- 0
x <- x + 1
print(x)
}
# Error in file.exists(code[1]) : file name conversion problem -- name too long?
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
?trace
trace(jieba_tokenizer, tracer = function() .count <<- .count + 1)
trace(jieba_tokenizer, tracer = function() .count <<- .count + 1)
lapply(docs, jieba_tokenizer)
trace(jieba_tokenizer, tracer = function() .count <<- .count + 1)
lapply(docs, jieba_tokenizer)
.count
untrace(jieba_tokenizer)
trace(jieba_tokenizer)
lapply(docs, jieba_tokenizer)
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
trace(jieba_tokenizer)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
# Error in file.exists(code[1]) : file name conversion problem -- name too long?
seg = lapply(docs, jieba_tokenizer)
seg = lapply(docs, jieba_tokenizer)
seg = lapply(docs, jieba_tokenizer)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
x = 0
x = x + 1
print(x)
}
# Error in file.exists(code[1]) : file name conversion problem -- name too long?
seg = lapply(docs, jieba_tokenizer)
Y1[21:22,]
View(docs)
docs[["21"]][["content"]]
docs[["22"]]
docs[["22"]][["content"]]
docs <- Corpus(VectorSource(Y1$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","》","♀","♂","『","』","【","】","／","＼","：","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
x = 0
x = x + 1
print(x)
}
# Error in file.exists(code[1]) : file name conversion problem -- name too long?
seg = lapply(docs, jieba_tokenizer)
View(docs)
docs[["21"]][["content"]]
docs[["22"]][["content"]]
# debug
Y1 <- Y1[-23,]
docs <- Corpus(VectorSource(Y1$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","》","♀","♂","『","』","【","】","／","＼","：","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
x = 0
x = x + 1
print(x)
}
# Error in file.exists(code[1]) : file name conversion problem -- name too long?
seg = lapply(docs, jieba_tokenizer)
View(docs)
docs[["18"]][["content"]]
docs[["20"]][["content"]]
# 依月份建立子資料組
Y1<- subset(data , data$month == "01", select = post)
docs[["24"]][["content"]]
docs[["23"]][["content"]]
View(seg)
seg[[1]]
docs[["1"]][["content"]]
freqFrame = as.data.frame(table(unlist(seg)))
# 清除單字
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) == 1){
freqFrame[i,] <- NA
}
}
freqFrame <- na.omit(freqFrame)
View(freqFrame)
png("Yao_Jan.png", width = 300, height = 300 )
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=5,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
# Error in strwidth(words[i], cex = size[i], ...) : 'cex'
dev.off()
