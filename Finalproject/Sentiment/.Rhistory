clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","，","。","！","「","（","」","）","\n","；")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
jieba_tokenizer = function(d){
unlist(segment(d[[1]], w1))
}
seg = lapply(docs, jieba_tokenizer)
# 計算情感分數
sapply(seg,function(d){
res <- d
temp<-data.frame()
temp[c(1:length(res)),1]<-rep('1.text' ,length(res)) #id
temp[c(1:length(res)),2]<-res[1:length(res)]#term
colnames(temp)<-c('id','term')
temp<-join(temp,posneg,by='term')
temp<-temp[!is.na(temp$weight),]
Ct_pos <- temp[temp$weight==1,3] %>% length()
Ct_neg <- temp[temp$weight==-1,3] %>% length()
return(Ct_pos/(Ct_pos+Ct_neg))
})
freqFrame = as.data.frame(table(unlist(seg)))
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
View(tdm)
View(tdm)
View(freqFrame)
d.corpus <- Corpus(VectorSource(seg))
View(d.corpus)
View(d.corpus)
View(docs)
View(d.corpus)
tdm <- TermDocumentMatrix(d.corpus)
tf <- as.matrix(tdm)
DF <- tidy(tf)
row.names(DF) <- DF$.rownames
Data <- DF[,1:10
Data <- DF[,1:10]
View(Data)
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
tf <- as.matrix(tdm)
DF <- tidy(tf)
row.names(DF) <- DF$.rownames
Data <- DF[,1:10]
View(Data)
library(topicmodels)
library("topicmodels")
install.packages("topicmodels")
library(topicmodels)
dtm_lda <- LDA(tdm, k = 16, control = list(seed = 1234))
View(docs)
View(dtm_lda)
library(ggplot2)
dtm_topics <- tidy(dtm_lda, matrix = "beta")
top_terms <- dtm_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
theme(axis.text.y=element_text(colour="black", family="Heiti TC Light"))
dtm <- DocumentTermMatrix(d.corpus)
d.corpus <- Corpus(VectorSource(seg))
dtm <- DocumentTermMatrix(d.corpus)
View(dtm)
dtm_lda <- LDA(dtm, k = 16, control = list(seed = 1234))
library(ggplot2)
dtm_topics <- tidy(dtm_lda, matrix = "beta")
top_terms <- dtm_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
theme(axis.text.y=element_text(colour="black", family="Heiti TC Light"))
docs <- clean_doc(docs)
library(jiebaR)
library(jiebaRD)
#### 使用情感字典 <NTUSD> ####
pos<-read.csv('NTUSD_positive_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
#### 使用情感字典 <NTUSD> ####
pos<-read.csv('NTUSD_positive_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(1, length(pos[,1])) #正面情感詞語權重為1
pos <- cbind(pos, weight)
neg<-read.csv('NTUSD_negative_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(-1, length(neg[,1])) #負面情感詞語權重為-1
posneg<-rbind(pos,neg)
neg <- cbind(neg, weight)
colnames(posneg)<-c('term','weight')
# 關閉pos 及 neg
rm(pos)
rm(neg)
rm(weight)
#### 使用情感字典 <NTUSD> ####
pos<-read.csv('NTUSD_positive_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(1, length(pos[,1])) #正面情感詞語權重為1
pos <- cbind(pos, weight)
neg<-read.csv('NTUSD_negative_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(-1, length(neg[,1])) #負面情感詞語權重為-1
neg <- cbind(neg, weight)
posneg<-rbind(pos,neg)
colnames(posneg)<-c('term','weight')
# 關閉pos 及 neg
rm(pos)
rm(neg)
rm(weight)
# 建立切分詞字典<NTUSD加入字典>及環境
user<-posneg[,'term']
w1<-worker()
new_user_word(w1,user)
# 使用測試資料
Data <- read.csv("Di_JanFebNews.csv")
# 套件引用
library(NLP)
library(stringr)
library(tm)
library(plyr)
# 文字清理
docs <- Corpus(VectorSource(Data$V3))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","，","。","！","「","（","」","）","\n","；")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
View(docs)
docs <- clean_doc(docs)
jieba_tokenizer = function(d){
unlist(segment(d[[1]], w1))
}
seg = lapply(docs, jieba_tokenizer)
View(seg)
# 計算情感分數
sapply(seg,function(d){
res <- d
temp<-data.frame()
temp[c(1:length(res)),1]<-rep('1.text' ,length(res)) #id
temp[c(1:length(res)),2]<-res[1:length(res)]#term
colnames(temp)<-c('id','term')
temp<-join(temp,posneg,by='term')
temp<-temp[!is.na(temp$weight),]
Ct_pos <- temp[temp$weight==1,3] %>% length()
Ct_neg <- temp[temp$weight==-1,3] %>% length()
return(Ct_pos/(Ct_pos+Ct_neg))
})
freqFrame = as.data.frame(table(unlist(seg)))
d.corpus <- Corpus(VectorSource(seg))
View(freqFrame)
d.corpus <- Corpus(VectorSource(seg))
View(d.corpus)
dtm <- DocumentTermMatrix(d.corpus)
library(topicmodels)
dtm_lda <- LDA(dtm, k = 4, control = list(seed = 1234))
library(ggplot2)
dtm_topics <- tidy(dtm_lda, matrix = "beta")
top_terms <- dtm_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
theme(axis.text.y=element_text(colour="black", family="Heiti TC Light"))
tdm <- TermDocumentMatrix(d.corpus)
tf <- as.matrix(tdm)
DF <- tidy(tf)
DF$.rownames
Sys.setlocale(locale="English")
dtm <- DocumentTermMatrix(d.corpus)
Sys.setlocale(locale="English")
tdm <- TermDocumentMatrix(d.corpus)
tdm <- TermDocumentMatrix(d.corpus)
dtm <- DocumentTermMatrix(d.corpus)
Sys.setlocale(category = "LC_ALL", locale = "cht")
tf <- as.matrix(tdm)
DF <- tidy(tf)
DF$.rownames
Sys.setlocale(category = "LC_ALL", locale = "cht")
tdm <- TermDocumentMatrix(d.corpus)
dtm <- DocumentTermMatrix(d.corpus)
Sys.setlocale(category = "LC_ALL", locale = "cht")
tf <- as.matrix(tdm)
DF <- tidy(tf)
DF$.rownames
Sys.setlocale(category='LC_ALL', locale='C')
tdm <- TermDocumentMatrix(d.corpus)
dtm <- DocumentTermMatrix(d.corpus)
Sys.setlocale(category = "LC_ALL", locale = "cht")
tf <- as.matrix(tdm)
DF <- tidy(tf)
DF$.rownames
Sys.setlocale(category = "LC_ALL", locale = "cht")
tf <- as.matrix(tdm)
DF <- tidy(tf)
DF$.rownames
tdm <- TermDocumentMatrix(d.corpus)
dtm <- DocumentTermMatrix(d.corpus)
Sys.setlocale(category = "LC_ALL", locale = "cht")
tf <- as.matrix(tdm)
DF <- tidy(tf)
DF$.rownames
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc)
{
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm))
{
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum))
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
# Take a look at a subset of tfidfnn
head(tfidfnn,10)
freq=rowSums(as.matrix(tfidfnn))
tail(freq,10)
plot(sort(freq, decreasing = T),col="blue",main="Word TF-IDF frequencies", xlab="TF-IDF-based rank", ylab = "TF-IDF")
plot(sort(freq, decreasing = T),col="blue",main="Word TF-IDF frequencies", xlab="TF-IDF-based rank", ylab = "TF-IDF")
high.freq=tail(sort(freq),n=20)
hfp.df=as.data.frame(sort(high.freq))
hfp.df$names <- rownames(hfp.df)
tail(sort(freq),n=20)
ggplot(hfp.df, aes(reorder(names,high.freq), high.freq)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Terms") + ylab("Frequency") +
ggtitle("Term frequencies")
library(jiebaR)
library(jiebaRD)
#### 使用情感字典 <NTUSD> ####
pos<-read.csv('NTUSD_positive_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(1, length(pos[,1])) #正面情感詞語權重為1
pos <- cbind(pos, weight)
neg<-read.csv('NTUSD_negative_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(-1, length(neg[,1])) #負面情感詞語權重為-1
neg <- cbind(neg, weight)
posneg<-rbind(pos,neg)
colnames(posneg)<-c('term','weight')
# 關閉pos 及 neg
rm(pos)
rm(neg)
rm(weight)
# 建立切分詞字典<NTUSD加入字典>及環境
user<-posneg[,'term']
w1<-worker()
new_user_word(w1,user)
setwd("~/GitHub/NTU-CSX-DataScience--Group5/Finalproject/Sentiment")
# 使用測試資料
Ko_Data <- read.csv("FB_result/Ko_report.csv")
View(Ko_Data)
# 套件引用
library(NLP)
library(stringr)
library(tm)
library(plyr)
# 文字清理
docs <- Corpus(VectorSource(Data$V3))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","，","。","！","「","（","」","）","\n","；")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
jieba_tokenizer = function(d){
unlist(segment(d[[1]], w1))
}
seg = lapply(docs, jieba_tokenizer)
# 計算情感分數
sapply(seg,function(d){
res <- d
temp<-data.frame()
temp[c(1:length(res)),1]<-rep('1.text' ,length(res)) #id
temp[c(1:length(res)),2]<-res[1:length(res)]#term
colnames(temp)<-c('id','term')
temp<-join(temp,posneg,by='term')
temp<-temp[!is.na(temp$weight),]
Ct_pos <- temp[temp$weight==1,3] %>% length()
Ct_neg <- temp[temp$weight==-1,3] %>% length()
return(Ct_pos/(Ct_pos+Ct_neg))
})
# 文字清理
docs <- Corpus(VectorSource(Ko_Data$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","，","。","！","「","（","」","）","\n","；")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
jieba_tokenizer = function(d){
unlist(segment(d[[1]], w1))
}
seg = lapply(docs, jieba_tokenizer)
# 計算情感分數
Ko_sentiment <- sapply(seg,function(d){
res <- d
temp<-data.frame()
temp[c(1:length(res)),1]<-rep('1.text' ,length(res)) #id
temp[c(1:length(res)),2]<-res[1:length(res)]#term
colnames(temp)<-c('id','term')
temp<-join(temp,posneg,by='term')
temp<-temp[!is.na(temp$weight),]
Ct_pos <- temp[temp$weight==1,3] %>% length()
Ct_neg <- temp[temp$weight==-1,3] %>% length()
return(Ct_pos/(Ct_pos+Ct_neg))
})
View(docs)
jieba_tokenizer = function(d){
unlist(segment(d[[1]], w1))
}
seg = lapply(docs, jieba_tokenizer)
View(seg)
View(Ko_Data)
# 使用測試資料
Ko_Data <- read.csv("FB_result/Ko_report.csv")
View(Ko_Data)
Ko_Data <- Ko_Data[-c(1:64),]
View(Ko_Data)
# 使用測試資料
Ko_Data <- read.csv("FB_result/Ko_report.csv")
View(docs)
View(docs)
View(Ko_Data)
Ko_Data <- Ko_Data[-c(1:12),]
View(Ko_Data)
Ko_Data[1,]
# 使用測試資料
Ko_Data <- read.csv("FB_result/Ko_report.csv")
# 使用測試資料
Ko_Data <- read.csv("FB_result/Ko_report.csv")
# 清除去年資料、6月資料、照片
Ko_Data <- Ko_Data[-c(139:200),]
Ko_Data <- Ko_Data[-64,]
Ko_Data <- Ko_Data[-c(1:12),]
View(Ko_Data)
# 套件引用
library(NLP)
library(stringr)
library(tm)
library(plyr)
# 文字清理
docs <- Corpus(VectorSource(Ko_Data$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","，","。","！","「","（","」","）","\n","；")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
jieba_tokenizer = function(d){
unlist(segment(d[[1]], w1))
}
seg = lapply(docs, jieba_tokenizer)
# 計算情感分數
Ko_sentiment <- sapply(seg,function(d){
res <- d
temp<-data.frame()
temp[c(1:length(res)),1]<-rep('1.text' ,length(res)) #id
temp[c(1:length(res)),2]<-res[1:length(res)]#term
colnames(temp)<-c('id','term')
temp<-join(temp,posneg,by='term')
temp<-temp[!is.na(temp$weight),]
Ct_pos <- temp[temp$weight==1,3] %>% length()
Ct_neg <- temp[temp$weight==-1,3] %>% length()
return(Ct_pos/(Ct_pos+Ct_neg))
})
Ko_sentiment
Ko_sentiment[is.na()]
Ko_sentiment %>% is.na
Ko_sentiment[Ko_sentiment %>% is.na]
Ko_sentiment[Ko_sentiment %>% is.na] = 0.5
Ko_sentiment
Ko_Data <- cbind(Ko_Data,Ko_sentiment)
View(Ko_Data)
library(jiebaR)
library(jiebaRD)
#### 使用情感字典 <NTUSD> ####
pos<-read.csv('NTUSD_positive_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(1, length(pos[,1])) #正面情感詞語權重為1
pos <- cbind(pos, weight)
neg<-read.csv('NTUSD_negative_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(-1, length(neg[,1])) #負面情感詞語權重為-1
neg <- cbind(neg, weight)
posneg<-rbind(pos,neg)
colnames(posneg)<-c('term','weight')
# 關閉pos 及 neg
rm(pos)
rm(neg)
rm(weight)
# 建立切分詞字典<NTUSD加入字典>及環境
user<-posneg[,'term']
w1<-worker()
new_user_word(w1,user)
#### 文字清理 ####
# 使用測試資料
Data <- read.csv("FB_result/Ko_report.csv")
# 清除去年資料、6月資料、照片
Data <- Data[-c(139:200),]
Data <- Data[-64,]
Data <- Data[-c(1:12),]
# 套件引用
library(NLP)
library(stringr)
library(tm)
library(plyr)
docs <- Corpus(VectorSource(Data$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","，","。","！","「","（","」","）","\n","；")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
# 開始切詞
jieba_tokenizer = function(d){
unlist(segment(d[[1]], w1))
}
seg = lapply(docs, jieba_tokenizer)
# 計算情感分數
sentiment_point <- sapply(seg,function(d){
res <- d
temp<-data.frame()
temp[c(1:length(res)),1]<-rep('1.text' ,length(res)) #id
temp[c(1:length(res)),2]<-res[1:length(res)]#term
colnames(temp)<-c('id','term')
temp<-join(temp,posneg,by='term')
temp<-temp[!is.na(temp$weight),]
Ct_pos <- temp[temp$weight==1,3] %>% length()
Ct_neg <- temp[temp$weight==-1,3] %>% length()
return(Ct_pos/(Ct_pos+Ct_neg))
})
Data <- cbind(Data,sentiment_point)
# 將NA改成0.5
sentiment_point[sentiment_point %>% is.na] = 0.5
Data <- cbind(Data,sentiment_point)
View(Data)
# 使用測試資料
Data <- read.csv("FB_result/Ko_report.csv")
# 清除去年資料、6月資料、照片
Data <- Data[-c(139:200),]
Data <- Data[-64,]
Data <- Data[-c(1:12),]
# 將NA改成0.5
sentiment_point[sentiment_point %>% is.na] = 0.5
Data <- cbind(Data,sentiment_point)
View(Data)
# 畫折線圖
economics
View(Data)
qplot(time, sentiment_point, data = Data, geom = "line")
qplot(share, sentiment_point, data = Data, geom = "line")
qplot(like, sentiment_point, data = Data, geom = "line")
qplot(like, shares, data = Data, geom = "line")
qplot(like, share, data = Data, geom = "line")
# 畫折線圖
economics
View(Data)
qplot(time, share, data = Data, geom = "line")
Data$time %>% as.Date()
qplot(time%>% as.Date(), share, data = Data, geom = "line")
qplot(time%>% as.Date(), sentient, data = Data, geom = "line")
qplot(time%>% as.Date(), sentient_point, data = Data, geom = "line")
qplot(time%>% as.Date(), sentiment_point, data = Data, geom = "line")
qplot(time%>% as.Date(), share,sentiment_point, data = Data, geom = "line")
qplot(time%>% as.Date(),sentiment_point, data = Data, geom = "line")
qplot(time%>% as.Date(),like, data = Data, geom = "line")
