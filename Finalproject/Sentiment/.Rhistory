x <- as.matrix(seq(-10, 10, length = 100))
x
y <- logistic(x) + rnorm(100, sd = 0.2)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
print(bpn)
#圖解BP
plot(bpn)
require(Metrics)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
y <- logistic(x) + rnorm(100, sd = 0.2)
#Loading the required packages
require(monmlp)
y <- logistic(x) + rnorm(100, sd = 0.2)
#Plotting Data
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
#Fitting Model
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 15, bag = TRUE)
mlpModel <- monmlp.predict(x = x, weights = mlpModel)
#Plotting predicted value over actual values
for(i in 1:15){
lines(x, attr(mlpModel, "ensemble")[[i]], col = "red")
}
cat ("MSE for Gradient Descent Trained Model: ", mse(y, mlpModel))
cat ("MSE for Gradient Descent Trained Model: ", mse(y, mlpModel))
#Clear the workspace
rm(list = ls())
#Loading the required packages
require(monmlp)
require(Metrics)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
y <- logistic(x) + rnorm(100, sd = 0.2)
#Plotting Data
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
#Fitting Model
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 15, bag = TRUE)
install.packages("kerasR")
iris
data <- iris
View(data)
dataset
datasets::airquality
#載入套件
library(neuralnet)
#整理資料
data <- iris
data$setosa <- ifelse(data$Species == "setosa", 1, 0)
data$versicolor <- ifelse(data$Species == "versicolor", 1, 0)
data$virginica <- ifelse(data$Species == "virginica", 1, 0)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
print(bpn)
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
#
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
#
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
#範例使用irisdata
data(iris)
#(2)分為訓練組和測試組資料集
set.seed(1117)
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
#訓練組樣本
traindata <- iris[t_idx,]
#測試組樣本
testdata <- iris[ - t_idx,]
#範例使用irisdata
data(iris)
#(2)分為訓練組和測試組資料集
set.seed(1117)
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
#訓練組樣本
traindata <- iris[t_idx,]
#測試組樣本
testdata <- iris[ - t_idx,]
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
install.packages("nnet")
library("nnet")
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
#(3)畫圖
plot.nnet(nnetM, wts.only = F)
#(4)預測
#test組執行預測
prediction <- predict(nnetM, testdata, type = 'class')
#預測結果
cm <- table(x = testdata$Species, y = prediction, dnn = c("實際", "預測"))
data <- iris
View(data)
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
#(3)畫圖
plot.nnet(nnetM, wts.only = F)
#預測結果
cm <- table(x = testdata$Species, y = prediction, dnn = c("實際", "預測"))
cm
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
View(testdata)
View(traindata)
################################################################
#   Differential expression analysis with limma
library(Biobase)
install.packages("GEOquery")
################################################################
#   Differential expression analysis with limma
library(Biobase)
install.packages("GEOquery")
gset <- getGEO("GSE19983", GSEMatrix =TRUE, AnnotGPL=FALSE)
install.packages("Biobase")
library(rvest)
library(magrittr)
library(httr)
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822")
View(test)
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_text()
test
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_text %>% html_nodes(.,"h1")
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,"h1") %>% html_text()
test
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,"h1") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".date") %>% html_text()
print("hello")
library(jsonlite)
library(httr)
library(magrittr)
library(dplyr)
setwd("~/GitHub/NTU-CSX-DataScience--Group5/Finalproject/FacebookAPI")
library(jsonlite)
library(httr)
library(magrittr)
library(dplyr)
token = "EAACEdEose0cBAHB1Dzz6MHZCa4guwG0Kx4z6SpCiA99g6GOu72NYZAAQSirSyLUVZCF7ckwW1OTJVVa6AvEe4dzyq949LJZCVqOftkw6LC4BjvmCXsHX0HypAd516mXQp0AZBoz0rilPme3NgGS3ynyD9ZAX8c1O48G7vSi3Nyyf0UMbdDAVx39ufZCXFeDtrcZD"
FacebookID = "tingshouchung"
## 注意 : limit請設定25的倍數
limit <- 200
# Crawl Posts data from facebookAPI 已完成開發
GetPost <- function(FacebookID,limit,token){
url_1 = "https://graph.facebook.com/v3.0/"
url_2 = "?fields=posts.limit("
url_3 = ")&access_token="
url = paste0(url_1,FacebookID,url_2,limit,url_3,token)
response = GET(url)
Posts  = content(response)
# Get posts/message from post <List>
Posts <- Posts$posts$data
# Get post data in data.frame -> post_data
post_data <- data.frame()
post_data<- sapply(Posts,function(data){
return(cbind(data$created_time,data$message))
}) %>% t
return(post_data)
}
GetShare <- function(FacebookID,limit,token){
url_1 = "https://graph.facebook.com/v3.0/"
url_2 = "?fields=posts.limit("
url_3 = "){shares}&access_token="
url = paste0(url_1,FacebookID,url_2,limit,url_3,token)
response = GET(url)
shares  = content(response)
shares <- shares$posts$data
shareCT <- c()
for(i in c(1:limit)){
shareCT <- c(shareCT,shares[[i]]$shares[[1]])
}
return(shareCT)
}
Getmood <- function(FacebookID,limit,token){
url_1 = "https://graph.facebook.com/v3.0/"
url_2 = "?fields=%20%20%20posts.as(like)%7Breactions.type(LIKE).limit(0).summary(true)%7D%2C%20%20%20posts.as(love)%7Breactions.type(LOVE).limit(0).summary(true)%7D%2C%20%20%20posts.as(wow)%7Breactions.type(WOW).limit(0).summary(true)%7D%2C%20%20%20posts.as(haha)%7Breactions.type(HAHA).limit(0).summary(true)%7D%2C%20%20%20posts.as(sad)%7Breactions.type(SAD).limit(0).summary(true)%7D%2C%20%20%20posts.as(angry)%7Breactions.type(ANGRY).limit(0).summary(true)%7D%2C%20%20%20posts.as(thankful)%7Breactions.type(THANKFUL).limit(0).summary(true)%7D&access_token="
url = paste0(url_1,FacebookID,url_2,token)
retext <- fromJSON(content(GET(url), "text"))
# Get first page mood count
like_temp <- (retext$like$data$reactions$summary %>% data.frame())$total_count
love_temp <- (retext$love$data$reactions$summary %>% data.frame())$total_count
haha_temp <- (retext$haha$data$reactions$summary %>% data.frame())$total_count
sad_temp <- (retext$sad$data$reactions$summary %>% data.frame())$total_count
wow_temp <- (retext$wow$data$reactions$summary %>% data.frame())$total_count
angry_temp <- (retext$angry$data$reactions$summary %>% data.frame())$total_count
mood_res <- cbind(like_temp,love_temp,haha_temp,sad_temp,wow_temp,angry_temp) %>% data.frame()
# Jump to nxt page
next_likeurl <- retext$like$paging$"next"
next_loveurl <- retext$love$paging$"next"
next_wowurl  <- retext$wow$paging$"next"
next_hahaurl <- retext$haha$paging$"next"
next_sadurl <- retext$sad$paging$"next"
next_angrurl <- retext$angry$paging$"next"
temp_limit <- limit
limit <- (limit-25)/25
library(tcltk) # 進度條
u <- 1:limit
pb <- tkProgressBar("進度","已完成 %", 0, 100)
for( i in 1:limit) {
info<- sprintf("已完成 %d%%", round(i*100/length(u)))
setTkProgressBar(pb, i*100/length(u), sprintf("進度 (%s)", info),info)
liketext <- fromJSON(content(GET(next_likeurl), "text"))
lovetext <- fromJSON(content(GET(next_loveurl), "text"))
wowtext <- fromJSON(content(GET(next_wowurl), "text"))
hahatext <- fromJSON(content(GET(next_hahaurl), "text"))
sadtext <- fromJSON(content(GET(next_sadurl), "text"))
angrtext <- fromJSON(content(GET(next_angrurl), "text"))
like_data <- (liketext$data$reactions$summary %>% data.frame())$total_count
love_data <- (lovetext$data$reactions$summary %>% data.frame())$total_count
wow_data  <- (wowtext$data$reactions$summary %>% data.frame())$total_count
haha_data <- (hahatext$data$reactions$summary %>% data.frame())$total_count
sad_data  <- (sadtext$data$reactions$summary %>% data.frame())$total_count
angr_data <- (angrtext$data$reactions$summary %>% data.frame())$total_count
like_temp <- c(like_temp,like_data)
love_temp <- c(love_temp,love_data)
wow_temp <- c(wow_temp,wow_data)
haha_temp <- c(haha_temp,haha_data)
sad_temp <- c(sad_temp,sad_data)
angry_temp <- c(angry_temp,angr_data)
next_likeurl <- liketext$paging$"next"
next_loveurl <- lovetext$paging$"next"
next_wowurl  <- wowtext$paging$"next"
next_hahaurl <- hahatext$paging$"next"
next_sadurl <-  sadtext$paging$"next"
next_angrurl <- angrtext$paging$"next"
}
close(pb)
mood_res <- cbind(like_temp,love_temp,haha_temp,sad_temp,wow_temp,angry_temp) %>% data.frame()
return(mood_res[1:temp_limit,])
}
Search_FB_post <- function(FacebookID,limit,token){
# 處理 limit非25的倍數問題 temp 儲存真實原始的limit數
temp_limit <- limit
if(limit%%25>0){
limit <- (ceiling(limit/25))*25
}
# 使用 已建立之 function 搜尋
Post_data <- GetPost(FacebookID,limit,token)
Share_data <- GetShare(FacebookID,limit,token)
Mood_data <- Getmood(FacebookID,limit,token)
Report <- cbind(Post_data,Share_data,Mood_data)
colnames(Report) <- c("time","post","share","like","love","haha","sad","wow","angry")
return(Report[1:temp_limit,])
}
Ko_FB_Report <- Search_FB_post(FacebookID,limit,token)
Mood_data <- Getmood(FacebookID,limit,token)
Ko_FB_Report[1,]
Share_data <- GetShare(FacebookID,limit,token)
Post_data <- GetPost(FacebookID,limit,token)
url_1 = "https://graph.facebook.com/v3.0/"
url_2 = "?fields=posts.limit("
url_3 = ")&access_token="
url = paste0(url_1,FacebookID,url_2,limit,url_3,token)
response = GET(url)
Posts  = content(response)
# Get posts/message from post <List>
Posts <- Posts$posts$data
Posts  = content(response)
View(Posts)
# Get posts/message from post <List>
Posts <- Posts$posts$data
View(Posts)
# Get post data in data.frame -> post_data
post_data <- data.frame()
post_data<- sapply(Posts,function(data){
return(cbind(data$created_time,data$message))
}) %>% t
Posts  = content(response)
# Get posts/message from post <List>
Posts <- Posts$posts$data
Posts  = content(response)
View(Posts)
View(Posts)
# Get posts/message from post <List>
Posts <- Posts$posts$data
View(Posts)
View(Posts)
# Get post data in data.frame -> post_data
post_data <- data.frame()
post_data<- sapply(Posts,function(data){
return(cbind(data$created_time,data$message))
}) %>% t
post_data<- sapply(Posts,function(data){
return(cbind(data$created_time,data$message))
})
# Get posts/message from post <List>
Posts <- Posts$posts$data
# Get post data in data.frame -> post_data
post_data <- data.frame()
post_data<- sapply(Posts,function(data){
return(cbind(data$created_time,data$message))
})
View(Post_data)
View(Post_data)
url_1 = "https://graph.facebook.com/v3.0/"
url_2 = "?fields=posts.limit("
url_3 = ")&access_token="
url = paste0(url_1,FacebookID,url_2,limit,url_3,token)
response = GET(url)
Posts  = content(response)
# Get posts/message from post <List>
Posts <- Posts$posts$data
View(Posts)
# Get post data in data.frame -> post_data
post_data <- data.frame()
post_data<- sapply(Posts,function(data){
return(cbind(data$created_time,data$message))
})
sapply(Posts,function(data){
return(data$created_time)
})
message <- sapply(Posts,function(data){
return(data$message)
})
time <- sapply(Posts,function(data){
return(data$created_time)
})
View(message)
View(Posts)
message <- sapply(Posts,function(data){
return(data$message %>% as.character())
})
message <- sapply(Posts,function(data){
return(data$message %>% as.character())
}) %>% unlist
message[100]
token = "EAACEdEose0cBAHB1Dzz6MHZCa4guwG0Kx4z6SpCiA99g6GOu72NYZAAQSirSyLUVZCF7ckwW1OTJVVa6AvEe4dzyq949LJZCVqOftkw6LC4BjvmCXsHX0HypAd516mXQp0AZBoz0rilPme3NgGS3ynyD9ZAX8c1O48G7vSi3Nyyf0UMbdDAVx39ufZCXFeDtrcZD"
## 注意 : limit請設定25的倍數
limit <- 200
FacebookID = "DoctorKoWJ"
url_1 = "https://graph.facebook.com/v3.0/"
url_2 = "?fields=%20%20%20posts.as(like)%7Breactions.type(LIKE).limit(0).summary(true)%7D%2C%20%20%20posts.as(love)%7Breactions.type(LOVE).limit(0).summary(true)%7D%2C%20%20%20posts.as(wow)%7Breactions.type(WOW).limit(0).summary(true)%7D%2C%20%20%20posts.as(haha)%7Breactions.type(HAHA).limit(0).summary(true)%7D%2C%20%20%20posts.as(sad)%7Breactions.type(SAD).limit(0).summary(true)%7D%2C%20%20%20posts.as(angry)%7Breactions.type(ANGRY).limit(0).summary(true)%7D%2C%20%20%20posts.as(thankful)%7Breactions.type(THANKFUL).limit(0).summary(true)%7D&access_token="
url = paste0(url_1,FacebookID,url_2,token)
retext <- fromJSON(content(GET(url), "text"))
# Get first page mood count
like_temp <- (retext$like$data$reactions$summary %>% data.frame())$total_count
love_temp <- (retext$love$data$reactions$summary %>% data.frame())$total_count
haha_temp <- (retext$haha$data$reactions$summary %>% data.frame())$total_count
sad_temp <- (retext$sad$data$reactions$summary %>% data.frame())$total_count
wow_temp <- (retext$wow$data$reactions$summary %>% data.frame())$total_count
angry_temp <- (retext$angry$data$reactions$summary %>% data.frame())$total_count
mood_res <- cbind(like_temp,love_temp,haha_temp,sad_temp,wow_temp,angry_temp) %>% data.frame()
url_1 = "https://graph.facebook.com/v3.0/"
url_2 = "?fields=posts.limit("
url_3 = ")&access_token="
url = paste0(url_1,FacebookID,url_2,limit,url_3,token)
response = GET(url)
Posts  = content(response)
# Get posts/message from post <List>
Posts <- Posts$posts$data
# Get post data in data.frame -> post_data
post_data <- data.frame()
time <- sapply(Posts,function(data){
return(data$created_time)
})
message <- sapply(Posts,function(data){
return(data$message %>% as.character())
}) %>% unlist
post_data <- cbind(time,message)
View(post_data)
token = "EAACEdEose0cBAHB1Dzz6MHZCa4guwG0Kx4z6SpCiA99g6GOu72NYZAAQSirSyLUVZCF7ckwW1OTJVVa6AvEe4dzyq949LJZCVqOftkw6LC4BjvmCXsHX0HypAd516mXQp0AZBoz0rilPme3NgGS3ynyD9ZAX8c1O48G7vSi3Nyyf0UMbdDAVx39ufZCXFeDtrcZD"
FacebookID = "DoctorKoWJ"
## 注意 : limit請設定25的倍數
limit <- 200
# Crawl Posts data from facebookAPI 已完成開發
GetPost <- function(FacebookID,limit,token){
url_1 = "https://graph.facebook.com/v3.0/"
url_2 = "?fields=posts.limit("
url_3 = ")&access_token="
url = paste0(url_1,FacebookID,url_2,limit,url_3,token)
response = GET(url)
Posts  = content(response)
# Get posts/message from post <List>
Posts <- Posts$posts$data
# Get post data in data.frame -> post_data
post_data <- data.frame()
time <- sapply(Posts,function(data){
return(data$created_time)
})
message <- sapply(Posts,function(data){
return(data$message %>% as.character())
}) %>% unlist
post_data <- cbind(time,message)
return(post_data)
}
Post_data <- GetPost(FacebookID,limit,token)
GetShare <- function(FacebookID,limit,token){
url_1 = "https://graph.facebook.com/v3.0/"
url_2 = "?fields=posts.limit("
url_3 = "){shares}&access_token="
url = paste0(url_1,FacebookID,url_2,limit,url_3,token)
response = GET(url)
shares  = content(response)
shares <- shares$posts$data
shareCT <- c()
for(i in c(1:limit)){
shareCT <- c(shareCT,shares[[i]]$shares[[1]])
}
return(shareCT)
}
Share_data <- GetShare(FacebookID,limit,token)
Getmood <- function(FacebookID,limit,token){
url_1 = "https://graph.facebook.com/v3.0/"
url_2 = "?fields=%20%20%20posts.as(like)%7Breactions.type(LIKE).limit(0).summary(true)%7D%2C%20%20%20posts.as(love)%7Breactions.type(LOVE).limit(0).summary(true)%7D%2C%20%20%20posts.as(wow)%7Breactions.type(WOW).limit(0).summary(true)%7D%2C%20%20%20posts.as(haha)%7Breactions.type(HAHA).limit(0).summary(true)%7D%2C%20%20%20posts.as(sad)%7Breactions.type(SAD).limit(0).summary(true)%7D%2C%20%20%20posts.as(angry)%7Breactions.type(ANGRY).limit(0).summary(true)%7D%2C%20%20%20posts.as(thankful)%7Breactions.type(THANKFUL).limit(0).summary(true)%7D&access_token="
url = paste0(url_1,FacebookID,url_2,token)
retext <- fromJSON(content(GET(url), "text"))
# Get first page mood count
like_temp <- (retext$like$data$reactions$summary %>% data.frame())$total_count
love_temp <- (retext$love$data$reactions$summary %>% data.frame())$total_count
haha_temp <- (retext$haha$data$reactions$summary %>% data.frame())$total_count
sad_temp <- (retext$sad$data$reactions$summary %>% data.frame())$total_count
wow_temp <- (retext$wow$data$reactions$summary %>% data.frame())$total_count
angry_temp <- (retext$angry$data$reactions$summary %>% data.frame())$total_count
mood_res <- cbind(like_temp,love_temp,haha_temp,sad_temp,wow_temp,angry_temp) %>% data.frame()
# Jump to nxt page
next_likeurl <- retext$like$paging$"next"
next_loveurl <- retext$love$paging$"next"
next_wowurl  <- retext$wow$paging$"next"
next_hahaurl <- retext$haha$paging$"next"
next_sadurl <- retext$sad$paging$"next"
next_angrurl <- retext$angry$paging$"next"
temp_limit <- limit
limit <- (limit-25)/25
library(tcltk) # 進度條
u <- 1:limit
pb <- tkProgressBar("進度","已完成 %", 0, 100)
for( i in 1:limit) {
info<- sprintf("已完成 %d%%", round(i*100/length(u)))
setTkProgressBar(pb, i*100/length(u), sprintf("進度 (%s)", info),info)
liketext <- fromJSON(content(GET(next_likeurl), "text"))
lovetext <- fromJSON(content(GET(next_loveurl), "text"))
wowtext <- fromJSON(content(GET(next_wowurl), "text"))
hahatext <- fromJSON(content(GET(next_hahaurl), "text"))
sadtext <- fromJSON(content(GET(next_sadurl), "text"))
angrtext <- fromJSON(content(GET(next_angrurl), "text"))
like_data <- (liketext$data$reactions$summary %>% data.frame())$total_count
love_data <- (lovetext$data$reactions$summary %>% data.frame())$total_count
wow_data  <- (wowtext$data$reactions$summary %>% data.frame())$total_count
haha_data <- (hahatext$data$reactions$summary %>% data.frame())$total_count
sad_data  <- (sadtext$data$reactions$summary %>% data.frame())$total_count
angr_data <- (angrtext$data$reactions$summary %>% data.frame())$total_count
like_temp <- c(like_temp,like_data)
love_temp <- c(love_temp,love_data)
wow_temp <- c(wow_temp,wow_data)
haha_temp <- c(haha_temp,haha_data)
sad_temp <- c(sad_temp,sad_data)
angry_temp <- c(angry_temp,angr_data)
next_likeurl <- liketext$paging$"next"
next_loveurl <- lovetext$paging$"next"
next_wowurl  <- wowtext$paging$"next"
next_hahaurl <- hahatext$paging$"next"
next_sadurl <-  sadtext$paging$"next"
next_angrurl <- angrtext$paging$"next"
}
close(pb)
mood_res <- cbind(like_temp,love_temp,haha_temp,sad_temp,wow_temp,angry_temp) %>% data.frame()
return(mood_res[1:temp_limit,])
}
Mood_data <- Getmood(FacebookID,limit,token)
Report <- cbind(Post_data,Share_data,Mood_data)
colnames(Report) <- c("time","post","share","like","love","haha","sad","wow","angry")
View(Report)
setwd("E:/GitHub/NTU-CSX-DataScience--Group5/Finalproject/Sentiment")
#### 使用情感字典 <NTUSD> ####
pos<-read.csv('NTUSD_positive_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(1, length(pos[,1])) #正面情感詞語權重為1
pos <- cbind(pos, weight)
weight <- rep(-1, length(neg[,1])) #負面情感詞語權重為-1
neg <- cbind(neg, weight)
neg<-read.csv('NTUSD_negative_unicode.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
posneg<-rbind(pos,neg)
colnames(posneg)<-c('term','weight')
# 關閉pos、neg及weight
rm(pos)
rm(neg)
rm(weight)
# 套件引用
library(NLP)
library(stringr)
library(tm)
library(plyr)
# 此時可以輸出Data這個結果
# 先做一些簡單的資料清理再輸出
data <- read.csv("FaceBookAPI-Taipei.csv")
View(data)
# 此時可以輸出Data這個結果
# 先做一些簡單的資料清理再輸出
data <- read.csv("News_3candi_4news.csv")
View(data)
data %>% summary
data$media %>% summary
