library(jiebaR)
library(magrittr)
library(RColorBrewer)
library(wordcloud)
# 匯入資料組
# 匯入資料組
setwd("/Users/Weber/Documents/GitHub/NTU-CSX-DataScience--Group5/Finalproject/LTN/Ko")
data_1 <- read.csv("Ko_JanLtnNews", encoding = "big5")
data_2 <- read.csv("Ko_FebLtnNews", encoding = "big5")
rm(list = ls())
library(httr)
library(rvest)
library(magrittr)
#檢視工作路徑以便存檔
getwd()
setwd("/Users/Weber/Documents/GitHub/NTU-CSX-DataScience--Group5/Finalproject/LTN/Ko")
#爬每頁網址的每一篇新聞的網址
#先做一個可以爬每頁網址的每篇新聞的function
FindLtnNews <- function(URL){
URL <- URL %>% as.character()
doc <- read_html(URL)
css <- "#newslistul > li > a.tit"
node <- html_nodes(doc, css)
link <- html_attrs(node)
#將news_links變成一個只有每篇新聞連結的list
news_links <- unlist(link)
news_links <- matrix(news_links , byrow = T , ncol = 3)
news_links <- news_links[ , 2 ]
news_links <- news_links %>% data.frame() %>% return()
}
#製作一個可以貼出完整每篇新聞網址的Function
PasteLtnlinks <- function(URL){
pre <-  "http://news.ltn.com.tw/"
URL <-   paste0(pre , URL) #%>% return() # Why不行?
return(URL)
}
#先寫個Function
FindNews <- function(URL){
URL <- URL %>% as.character()
t_title <- read_html(URL) %>% html_nodes(.,"h1") %>% html_text()
title <- rbind(title, t_title)
t_time <- read_html(URL) %>% html_node(.,".date") %>% html_text()
time <- rbind(time, t_time)
t_text <- read_html(URL) %>% html_nodes(.,"p") %>% html_text()
s <- length(t_text) - 5
ss <- t_text[1:s]
bindtext <- ""
for(i in c(1:length(ss))){
bindtext <- paste(bindtext,ss[i])
}
return(cbind(title, time ,bindtext))
}
#==========================================
#爬柯文哲二月新聞的網頁網址
ltn_feb_url <- "http://news.ltn.com.tw/search/?keyword=%E6%9F%AF%E6%96%87%E5%93%B2&conditions=and&SYear=2018&SMonth=2&SDay=1&EYear=2018&EMonth=2&EDay=28&page="
feb_ltnurl <- c()
#22頁只是2/1-2/28
for( i in 1: 22) {
print(i)
url <- paste0(ltn_feb_url , i )
feb_ltnurl <- rbind(feb_ltnurl , url )
Sys.sleep(sample(1:5 , 1))
}
#存柯文哲二月新聞的網頁網址
write.csv(feb_ltnurl , "Ko_febltnpage")
Ko_febltnpage <-read.csv("Ko_febltnpage")
Ko_febltnpage <- Ko_febltnpage[ , 2] #只有第二欄是網址需要
#用上面的Function爬二月的每篇新聞的網址
output4 <- c()
for ( i in 1:length(Ko_febltnpage)) {
print(i)
runlinks <- FindLtnNews(Ko_febltnpage[i])
output4 <- rbind(output4, runlinks)
Sys.sleep(sample(1:5,1))
}
Ko_FebLtnlink <- c()
for ( n in 1:length(output4$.)) {
print(n)
URL <- PasteLtnlinks(output4$.[n])
Ko_FebLtnlink <- rbind( Ko_FebLtnlink, URL)
}
#將每篇新聞的網址存檔
write.csv(Ko_FebLtnlink, file = "Ko_FebLtnlink")
#讀檔
Ko_FebLtnlink <- read.csv("Ko_FebLtnlink")
#讀每篇新聞的標題時間內文
title <- c()
time <- c()
text <- c()
#開始爬二月每篇新聞的標題時間內文
Ko_FebLtnNews <- data.frame()
for ( m in 1:length(Ko_FebLtnlink$V1)) {
print(m)
output <- FindNews(Ko_FebLtnlink$V1[m])
output <- cbind(output, Ko_FebLtnlink$V1[m])
Ko_FebLtnNews <- rbind(Ko_FebLtnNews, output)
Sys.sleep(sample(1:10, 1))
}
#存二月每篇新聞的標題時間內文連結
write.csv(Ko_FebLtnNews, "Ko_FebLtnNews")
# ===== 文字雲程式
# ===== 處理柯文哲1月至5月的文章
# ------------------------------------------
# 遇到問題#1 :
# 每篇首尾都是規律的廢話，不過有少部分沒有廢話，所以無法用substr()一次清乾淨
# 解決方法#1 :
# 用toSpace函數與tm_map功能換掉廢話，不過又衍生新的問題(已解決)
# 匯入套件
library(dplyr)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(magrittr)
library(RColorBrewer)
library(wordcloud)
# 匯入資料組
setwd("/Users/Weber/Documents/GitHub/NTU-CSX-DataScience--Group5/Finalproject/LTN/Ko")
data_2 <- read.csv("Ko_FebLtnNews", encoding = "big5")
#====== 2月切詞
docs <- Corpus(VectorSource(data_2$bindtext))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs, toSpace, " 為達最佳瀏覽效果，建議使用 Chrome、Firefox 或 Internet Explorer 10")
docs <- tm_map(docs, toSpace, "新聞送上來！快加入自由電子報APP、LINE好友  2018年6月13日‧星期三‧戊戌年四月卅 注目新聞 1 中國卡車罷工延燒 司機高喊「打倒共產黨」！ 2 新版身分證票選出爐 設計師魯少綸奪首獎！ 3 洋腸又來！夜店外帶18歲妹 網站取名「台女很容易」... 4 侯友宜妻1塊土地99個門牌 蘇貞昌：太荒唐 5 連俞涵超正弟媳曝光…根本巨乳林志玲！")
docs <- tm_map(docs, toSpace, "以上版本的瀏覽器。 爆")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs, toSpace, "、")
docs <- tm_map(docs, toSpace, "，")
docs <- tm_map(docs, toSpace, "。")
docs <- tm_map(docs, toSpace, "！")
docs <- tm_map(docs, toSpace, "「")
docs <- tm_map(docs, toSpace, "（")
docs <- tm_map(docs, toSpace, "」")
docs <- tm_map(docs, toSpace, "）")
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, toSpace, "；")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "也")
docs <- tm_map(docs, toSpace, "都")
docs <- tm_map(docs, toSpace, "就")
docs <- tm_map(docs, toSpace, "與")
docs <- tm_map(docs, toSpace, "但")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "在")
docs <- tm_map(docs, toSpace, "和")
docs <- tm_map(docs, toSpace, "及")
docs <- tm_map(docs, toSpace, "為")
docs <- tm_map(docs, toSpace, "或")
docs <- tm_map(docs, toSpace, "且")
docs <- tm_map(docs, toSpace, "有")
docs <- tm_map(docs, toSpace, "含")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
#有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 畫出文字雲
# 儲存文字雲圖片
png("Ko_Feb.png", width = 500, height = 500 )
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=50,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
dev.off()
# ===== 文字雲程式
# ===== 處理柯文哲1月至5月的文章
# ------------------------------------------
# 遇到問題#1 :
# 每篇首尾都是規律的廢話，不過有少部分沒有廢話，所以無法用substr()一次清乾淨
# 解決方法#1 :
# 用toSpace函數與tm_map功能換掉廢話，不過又衍生新的問題(已解決)
# 匯入套件
library(dplyr)
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
library(magrittr)
library(RColorBrewer)
library(wordcloud)
# 匯入資料組
setwd("/Users/Weber/Documents/GitHub/NTU-CSX-DataScience--Group5/Finalproject/LTN/Ko")
data_1 <- read.csv("Ko_JanLtnNews", encoding = "big5")
data_2 <- read.csv("Ko_FebLtnNews", encoding = "big5")
data_3 <- read.csv("Ko_MarLtnNews", encoding = "big5")
data_4 <- read.csv("Ko_AprLtnNews", encoding = "big5")
data_5 <- read.csv("Ko_MayLtnNews", encoding = "big5")
# Di_text <- matrix(data = NA, nrow = 67,ncol = 5 )
# Di_text <- cbind(data_1$bindtext, data_2$bindtext, data_3$bindtext, data_4$bindtext, data_5$bindtext)
# ----------------------------------------------------------------------------------------------------------
# 清理文本內容，刪除文本中不需要的雜訊
# data_i$bindtext <- substr(data$bindtext, 69, (nchar(as.vector(data$bindtext))-183))
# data$bindtext <- substr(data$bindtext, 69, (nchar(as.vector(data$bindtext))-183))
# 切詞
# data_1$bindtext[1:3] <- substr(data_1$bindtext[1:3], 69, (nchar(as.vector(data_1$bindtext[1:3]))-176))
# ----------------------------------------------------------------------------------------------------------
# 以上內容有待開發
#====== 1月切詞
docs <- Corpus(VectorSource(data_1$bindtext))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs, toSpace, " 為達最佳瀏覽效果，建議使用 Chrome、Firefox 或 Internet Explorer 10")
docs <- tm_map(docs, toSpace, "新聞送上來！快加入自由電子報APP、LINE好友  2018年6月13日‧星期三‧戊戌年四月卅 注目新聞 1 中國卡車罷工延燒 司機高喊「打倒共產黨」！ 2 新版身分證票選出爐 設計師魯少綸奪首獎！ 3 洋腸又來！夜店外帶18歲妹 網站取名「台女很容易」... 4 侯友宜妻1塊土地99個門牌 蘇貞昌：太荒唐 5 連俞涵超正弟媳曝光…根本巨乳林志玲！")
docs <- tm_map(docs, toSpace, "以上版本的瀏覽器。 爆")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs, toSpace, "、")
docs <- tm_map(docs, toSpace, "，")
docs <- tm_map(docs, toSpace, "。")
docs <- tm_map(docs, toSpace, "！")
docs <- tm_map(docs, toSpace, "「")
docs <- tm_map(docs, toSpace, "（")
docs <- tm_map(docs, toSpace, "」")
docs <- tm_map(docs, toSpace, "）")
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, toSpace, "；")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "也")
docs <- tm_map(docs, toSpace, "都")
docs <- tm_map(docs, toSpace, "就")
docs <- tm_map(docs, toSpace, "與")
docs <- tm_map(docs, toSpace, "但")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "在")
docs <- tm_map(docs, toSpace, "和")
docs <- tm_map(docs, toSpace, "及")
docs <- tm_map(docs, toSpace, "為")
docs <- tm_map(docs, toSpace, "或")
docs <- tm_map(docs, toSpace, "且")
docs <- tm_map(docs, toSpace, "有")
docs <- tm_map(docs, toSpace, "含")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
#有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 畫出文字雲
# 儲存文字雲圖片
png("Ko_Jan.png", width = 500, height = 500 )
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=70,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
dev.off()
#====== 2月切詞
docs <- Corpus(VectorSource(data_2$bindtext))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs, toSpace, " 為達最佳瀏覽效果，建議使用 Chrome、Firefox 或 Internet Explorer 10")
docs <- tm_map(docs, toSpace, "新聞送上來！快加入自由電子報APP、LINE好友  2018年6月13日‧星期三‧戊戌年四月卅 注目新聞 1 中國卡車罷工延燒 司機高喊「打倒共產黨」！ 2 新版身分證票選出爐 設計師魯少綸奪首獎！ 3 洋腸又來！夜店外帶18歲妹 網站取名「台女很容易」... 4 侯友宜妻1塊土地99個門牌 蘇貞昌：太荒唐 5 連俞涵超正弟媳曝光…根本巨乳林志玲！")
docs <- tm_map(docs, toSpace, "以上版本的瀏覽器。 爆")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs, toSpace, "、")
docs <- tm_map(docs, toSpace, "，")
docs <- tm_map(docs, toSpace, "。")
docs <- tm_map(docs, toSpace, "！")
docs <- tm_map(docs, toSpace, "「")
docs <- tm_map(docs, toSpace, "（")
docs <- tm_map(docs, toSpace, "」")
docs <- tm_map(docs, toSpace, "）")
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, toSpace, "；")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "也")
docs <- tm_map(docs, toSpace, "都")
docs <- tm_map(docs, toSpace, "就")
docs <- tm_map(docs, toSpace, "與")
docs <- tm_map(docs, toSpace, "但")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "在")
docs <- tm_map(docs, toSpace, "和")
docs <- tm_map(docs, toSpace, "及")
docs <- tm_map(docs, toSpace, "為")
docs <- tm_map(docs, toSpace, "或")
docs <- tm_map(docs, toSpace, "且")
docs <- tm_map(docs, toSpace, "有")
docs <- tm_map(docs, toSpace, "含")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
#有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 畫出文字雲
# 儲存文字雲圖片
png("Ko_Feb.png", width = 500, height = 500 )
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=70,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
dev.off()
#====== 3月切詞
docs <- Corpus(VectorSource(data_3$bindtext))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs, toSpace, " 為達最佳瀏覽效果，建議使用 Chrome、Firefox 或 Internet Explorer 10")
docs <- tm_map(docs, toSpace, "新聞送上來！快加入自由電子報APP、LINE好友  2018年6月13日‧星期三‧戊戌年四月卅 注目新聞 1 中國卡車罷工延燒 司機高喊「打倒共產黨」！ 2 新版身分證票選出爐 設計師魯少綸奪首獎！ 3 洋腸又來！夜店外帶18歲妹 網站取名「台女很容易」... 4 侯友宜妻1塊土地99個門牌 蘇貞昌：太荒唐 5 連俞涵超正弟媳曝光…根本巨乳林志玲！")
docs <- tm_map(docs, toSpace, "以上版本的瀏覽器。 爆")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs, toSpace, "、")
docs <- tm_map(docs, toSpace, "，")
docs <- tm_map(docs, toSpace, "。")
docs <- tm_map(docs, toSpace, "！")
docs <- tm_map(docs, toSpace, "「")
docs <- tm_map(docs, toSpace, "（")
docs <- tm_map(docs, toSpace, "」")
docs <- tm_map(docs, toSpace, "）")
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, toSpace, "；")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "也")
docs <- tm_map(docs, toSpace, "都")
docs <- tm_map(docs, toSpace, "就")
docs <- tm_map(docs, toSpace, "與")
docs <- tm_map(docs, toSpace, "但")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "在")
docs <- tm_map(docs, toSpace, "和")
docs <- tm_map(docs, toSpace, "及")
docs <- tm_map(docs, toSpace, "為")
docs <- tm_map(docs, toSpace, "或")
docs <- tm_map(docs, toSpace, "且")
docs <- tm_map(docs, toSpace, "有")
docs <- tm_map(docs, toSpace, "含")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
#有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 畫出文字雲
# 儲存文字雲圖片
png("Ko_Mar.png", width = 500, height = 500 )
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=70,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
dev.off()
#====== 4月切詞
docs <- Corpus(VectorSource(data_4$bindtext))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs, toSpace, " 為達最佳瀏覽效果，建議使用 Chrome、Firefox 或 Internet Explorer 10")
docs <- tm_map(docs, toSpace, "新聞送上來！快加入自由電子報APP、LINE好友  2018年6月13日‧星期三‧戊戌年四月卅 注目新聞 1 中國卡車罷工延燒 司機高喊「打倒共產黨」！ 2 新版身分證票選出爐 設計師魯少綸奪首獎！ 3 洋腸又來！夜店外帶18歲妹 網站取名「台女很容易」... 4 侯友宜妻1塊土地99個門牌 蘇貞昌：太荒唐 5 連俞涵超正弟媳曝光…根本巨乳林志玲！")
docs <- tm_map(docs, toSpace, "以上版本的瀏覽器。 爆")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs, toSpace, "、")
docs <- tm_map(docs, toSpace, "，")
docs <- tm_map(docs, toSpace, "。")
docs <- tm_map(docs, toSpace, "！")
docs <- tm_map(docs, toSpace, "「")
docs <- tm_map(docs, toSpace, "（")
docs <- tm_map(docs, toSpace, "」")
docs <- tm_map(docs, toSpace, "）")
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, toSpace, "；")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "也")
docs <- tm_map(docs, toSpace, "都")
docs <- tm_map(docs, toSpace, "就")
docs <- tm_map(docs, toSpace, "與")
docs <- tm_map(docs, toSpace, "但")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "在")
docs <- tm_map(docs, toSpace, "和")
docs <- tm_map(docs, toSpace, "及")
docs <- tm_map(docs, toSpace, "為")
docs <- tm_map(docs, toSpace, "或")
docs <- tm_map(docs, toSpace, "且")
docs <- tm_map(docs, toSpace, "有")
docs <- tm_map(docs, toSpace, "含")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
#有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 畫出文字雲
# 儲存文字雲圖片
png("Ko_Apr.png", width = 500, height = 500 )
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=70,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
dev.off()
#====== 5月切詞
docs <- Corpus(VectorSource(data_5$bindtext))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs, toSpace, " 為達最佳瀏覽效果，建議使用 Chrome、Firefox 或 Internet Explorer 10")
docs <- tm_map(docs, toSpace, "新聞送上來！快加入自由電子報APP、LINE好友  2018年6月13日‧星期三‧戊戌年四月卅 注目新聞 1 中國卡車罷工延燒 司機高喊「打倒共產黨」！ 2 新版身分證票選出爐 設計師魯少綸奪首獎！ 3 洋腸又來！夜店外帶18歲妹 網站取名「台女很容易」... 4 侯友宜妻1塊土地99個門牌 蘇貞昌：太荒唐 5 連俞涵超正弟媳曝光…根本巨乳林志玲！")
docs <- tm_map(docs, toSpace, "以上版本的瀏覽器。 爆")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs, toSpace, "、")
docs <- tm_map(docs, toSpace, "，")
docs <- tm_map(docs, toSpace, "。")
docs <- tm_map(docs, toSpace, "！")
docs <- tm_map(docs, toSpace, "「")
docs <- tm_map(docs, toSpace, "（")
docs <- tm_map(docs, toSpace, "」")
docs <- tm_map(docs, toSpace, "）")
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, toSpace, "；")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "也")
docs <- tm_map(docs, toSpace, "都")
docs <- tm_map(docs, toSpace, "就")
docs <- tm_map(docs, toSpace, "與")
docs <- tm_map(docs, toSpace, "但")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "在")
docs <- tm_map(docs, toSpace, "和")
docs <- tm_map(docs, toSpace, "及")
docs <- tm_map(docs, toSpace, "為")
docs <- tm_map(docs, toSpace, "或")
docs <- tm_map(docs, toSpace, "且")
docs <- tm_map(docs, toSpace, "有")
docs <- tm_map(docs, toSpace, "含")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
#有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 畫出文字雲
# 儲存文字雲圖片
png("Ko_May.png", width = 500, height = 500 )
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=70,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
dev.off()
